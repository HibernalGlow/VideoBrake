"""
æ“ä½œæ‰§è¡Œæ¨¡å— - è´Ÿè´£æ‰§è¡Œå…·ä½“çš„æ–‡ä»¶æ“ä½œ
"""
import os
import concurrent.futures
import time
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional, Union
import pyperclip

from rich.console import Console
from rich.progress import Progress, TextColumn, BarColumn, SpinnerColumn
from rich.progress import TimeElapsedColumn, TimeRemainingColumn
from rich.panel import Panel

# å¯¼å…¥é…ç½®å¤„ç†
from .config import get_video_extensions, get_prefix_list, get_prefix_by_name, get_output_filename

# è®¾ç½®æ§åˆ¶å°å¯¹è±¡
console = Console()


def process_single_file(file_path: str, add_nov: bool = True) -> Tuple[bool, str]:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œæ·»åŠ æˆ–ç§»é™¤.novåç¼€
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        add_nov: æ˜¯å¦æ·»åŠ .novåç¼€ï¼ˆTrueä¸ºæ·»åŠ ï¼ŒFalseä¸ºç§»é™¤ï¼‰
        
    Returns:
        Tuple[bool, str]: (æ“ä½œæ˜¯å¦æˆåŠŸ, æ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯æ¶ˆæ¯)
    """
    try:
        # è·å–åŸå§‹æ–‡ä»¶çš„æ—¶é—´æˆ³
        stat = os.stat(file_path)
        atime = stat.st_atime  # è®¿é—®æ—¶é—´
        mtime = stat.st_mtime  # ä¿®æ”¹æ—¶é—´
        
        # æ ¹æ®æ“ä½œç±»å‹é‡å‘½åæ–‡ä»¶
        new_path = file_path + '.nov' if add_nov else file_path[:-4]
        os.rename(file_path, new_path)
        
        # æ¢å¤æ—¶é—´æˆ³
        os.utime(new_path, (atime, mtime))
        return True, file_path
    except Exception as e:
        return False, f'é”™è¯¯ {file_path}: {e}'


def check_and_save_duplicates(output_dir: str, scan_results: Dict[str, Any], prefix_name: str = "hb") -> Dict[str, Any]:
    """
    æ£€æŸ¥å¸¦ç‰¹å®šå‰ç¼€çš„æ–‡ä»¶å¯¹åº”çš„æ— å‰ç¼€æ–‡ä»¶ï¼Œå¹¶ä¿å­˜æ— å‰ç¼€æ–‡ä»¶è·¯å¾„
    
    Args:
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        scan_results: æ‰«æç»“æœæ•°æ®
        prefix_name: è¦æ£€æŸ¥çš„å‰ç¼€åç§°ï¼Œé»˜è®¤ä¸º"hb"
    """
    # è·å–å‰ç¼€ä¿¡æ¯ - æ ¹æ®åç§°æŸ¥æ‰¾å¯¹åº”çš„å‰ç¼€é…ç½®
    prefixes = get_prefix_list()
    prefix_info = None
    for p in prefixes:
        if p.get("name") == prefix_name:
            prefix_info = p
            break
    
    if not prefix_info:
        console.print(f"\n[yellow]âš ï¸ æœªæ‰¾åˆ°åä¸º '{prefix_name}' çš„å‰ç¼€é…ç½®[/yellow]")
        return {
            "duplicates": [],
            "pairs": [],
            "prefixed_larger": []
        }
    
    # è·å–å‰ç¼€å­—ç¬¦ä¸²å’Œæè¿°
    prefix = prefix_info.get("prefix")
    description = prefix_info.get("description", f"{prefix_name} å‰ç¼€æ–‡ä»¶")
    
    # è·å–å‰ç¼€æ–‡ä»¶ - ä½¿ç”¨å‰ç¼€åç§°(name)ä½œä¸ºé”®
    prefixed_files = scan_results.get("prefixed_files", {}).get(prefix_name, [])
    if not prefixed_files:
        console.print(f"\n[yellow]æ²¡æœ‰æ‰¾åˆ°å¸¦ '{prefix}' å‰ç¼€çš„è§†é¢‘æ–‡ä»¶ï¼Œæ— éœ€æ£€æŸ¥ã€‚[/yellow]")
        return {
            "duplicates": [],
            "pairs": [],
            "prefixed_larger": []
        }
    
    nov_files = scan_results.get("nov_files", [])
    normal_files = scan_results.get("normal_files", [])
    
    # åŒ¹é…é€»è¾‘
    duplicate_non_prefixed_files: List[str] = []
    # {(directory, base_name_without_ext): {"actual": existing_file_path, "logical": output_path}}
    non_prefixed_lookup: Dict[Tuple[str, str], Dict[str, str]] = {}
    matched_pairs: List[Tuple[str, str]] = []  # (prefixed_path, original_actual_path)
    prefixed_larger: List[Tuple[str, str, int, int]] = []  # (prefixed, original, size_prefixed, size_original)
    
    # å»ºç«‹æ— å‰ç¼€æ–‡ä»¶çš„æŸ¥æ‰¾å­—å…¸
    for f in normal_files:
        p = Path(f)
        key = (str(p.parent), p.stem)
        non_prefixed_lookup[key] = {"actual": f, "logical": f}
    
    for f in nov_files:
        p = Path(f)
        original_name_path = Path(p.stem)  # è·å– .nov å‰çš„æ–‡ä»¶åï¼ˆå«åŸå§‹æ‰©å±•åï¼‰
        key = (str(p.parent), original_name_path.stem)  # è·å–ä¸å«æ‰©å±•åçš„åŸºç¡€å
        # å®é™…å­˜åœ¨çš„æ–‡ä»¶æ˜¯å¸¦ .nov çš„è·¯å¾„ï¼Œç”¨äºå¤§å°æ¯”è¾ƒï¼›è¾“å‡ºæ—¶å¸Œæœ›å±•ç¤ºæ¢å¤å‰çš„é€»è¾‘è·¯å¾„
        non_prefixed_lookup[key] = {"actual": f, "logical": f[:-4]}
    
    # æŸ¥æ‰¾åŒ¹é…é¡¹
    for prefixed_file in prefixed_files:
        prefixed_path = Path(prefixed_file)
        prefixed_dir = str(prefixed_path.parent)
        prefixed_name = prefixed_path.name
        
        # å»æ‰å‰ç¼€ï¼Œè·å–åŸºç¡€åï¼ˆä¸å«æ‰©å±•åï¼‰
        base_name_with_ext = prefixed_name[len(prefix):]
        base_name_without_ext = Path(base_name_with_ext).stem
        lookup_key = (prefixed_dir, base_name_without_ext)
        
        if lookup_key in non_prefixed_lookup:
            original_paths = non_prefixed_lookup[lookup_key]
            duplicate_non_prefixed_files.append(original_paths["logical"])  # è¾“å‡ºåˆ—è¡¨ä½¿ç”¨é€»è¾‘è·¯å¾„
            matched_pairs.append((prefixed_file, original_paths["actual"]))

            # æ¯”è¾ƒå¤§å°ï¼ˆä»…å½“ä¸¤ä¸ªè·¯å¾„éƒ½å­˜åœ¨æ—¶ï¼‰
            try:
                if os.path.exists(prefixed_file) and os.path.exists(original_paths["actual"]):
                    size_pref = os.path.getsize(prefixed_file)
                    size_orig = os.path.getsize(original_paths["actual"])
                    if size_pref > size_orig:
                        prefixed_larger.append((prefixed_file, original_paths["actual"], size_pref, size_orig))
            except Exception:
                # å¿½ç•¥å•ä¸ªå¤§å°æ¯”è¾ƒé”™è¯¯ï¼Œç»§ç»­å…¶ä»–é¡¹
                pass
    
    # å¤„ç†ç»“æœ
    if duplicate_non_prefixed_files:
        output_filename = get_output_filename()
        output_path = os.path.join(output_dir, output_filename)
        try:
            # å»é‡å¹¶æ’åº
            unique_duplicates = sorted(list(set(duplicate_non_prefixed_files)))
            
            # å°†ç»“æœå†™å…¥æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                for file_path in unique_duplicates:
                    f.write(file_path + '\n')
            
            # å°†ç»“æœå¤åˆ¶åˆ°å‰ªè´´æ¿
            clipboard_content = '\n'.join(unique_duplicates)
            pyperclip.copy(clipboard_content)
            
            console.print(f"\n[green]âœ“ å‘ç° {len(unique_duplicates)} ä¸ªå¯¹åº”çš„æ— å‰ç¼€æ–‡ä»¶[/green]")
            console.print(f"[green]âœ“ åˆ—è¡¨å·²ä¿å­˜åˆ°:[/green] {output_path}")
            console.print(f"[green]âœ“ é‡å¤æ–‡ä»¶è·¯å¾„å·²å¤åˆ¶åˆ°å‰ªè´´æ¿![/green]")
        except Exception as e:
            console.print(f"\n[red]âœ— é”™è¯¯ï¼šæ— æ³•å†™å…¥é‡å¤æ–‡ä»¶åˆ—è¡¨åˆ° {output_path}: {e}[/red]")
    else:
        console.print(f"\n[green]âœ“ æœªå‘ç° '{prefix}' æ–‡ä»¶å¯¹åº”çš„æ— å‰ç¼€é‡å¤æ–‡ä»¶ã€‚[/green]")

    # æ‰“å°â€œå‰ç¼€æ–‡ä»¶æ›´å¤§â€çš„æ±‡æ€»åˆ—è¡¨
    if prefixed_larger:
        console.print(f"\n[yellow]âš ï¸ ä»¥ä¸‹ {len(prefixed_larger)} å¯¹ä¸­ï¼Œå¸¦å‰ç¼€æ–‡ä»¶ä½“ç§¯å¤§äºåŸè§†é¢‘ï¼ˆä»…åŒç›®å½•é…å¯¹ï¼Œå·²å¿½ç•¥æ ¼å¼å·®å¼‚ï¼‰ï¼š[/yellow]")
        for pref, orig, sp, so in prefixed_larger:
            try:
                def _fmt(n: int) -> str:
                    for unit in ["B", "KB", "MB", "GB", "TB"]:
                        if n < 1024 or unit == "TB":
                            return f"{n:.1f}{unit}" if unit != "B" else f"{n}B"
                        n /= 1024
                    return f"{n}B"
                console.print(f"  [yellow]â€¢[/yellow] [+] {Path(pref).name} ({_fmt(sp)}) > {Path(orig).name} ({_fmt(so)})")
                console.print(f"    pref: {pref}")
                console.print(f"    orig: {orig}")
            except Exception:
                # å³ä¾¿æ ¼å¼åŒ–å¤±è´¥ä¹Ÿä¸å½±å“ä¸»æµç¨‹
                console.print(f"  [yellow]â€¢[/yellow] [+] {pref} > {orig}")

    return {
        "duplicates": duplicate_non_prefixed_files,
        "pairs": matched_pairs,
        "prefixed_larger": prefixed_larger,
    }


def add_nov_extension_to_files(files: List[str]) -> Tuple[int, List[str]]:
    """
    ä¸ºæ–‡ä»¶åˆ—è¡¨æ·»åŠ .novåç¼€
    
    Args:
        files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        Tuple[int, List[str]]: (æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°é‡, é”™è¯¯æ¶ˆæ¯åˆ—è¡¨)
    """
    success_count = 0
    error_files = []
    
    # è¿‡æ»¤æ‰å·²ç»æœ‰.novåç¼€çš„æ–‡ä»¶
    files_to_process = [f for f in files if not f.lower().endswith('.nov')]
    
    if not files_to_process:
        return 0, []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
        console=console
    ) as progress:
        task = progress.add_task(f"æ­£åœ¨æ·»åŠ .novæ‰©å±•å...", total=len(files_to_process))
        
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = [executor.submit(process_single_file, file, True) for file in files_to_process]
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                result, file_or_error = future.result()
                if result:
                    success_count += 1
                else:
                    error_files.append(file_or_error)
                progress.advance(task)
    
    return success_count, error_files


def remove_nov_extension_from_files(files: List[str]) -> Tuple[int, List[str]]:
    """
    ç§»é™¤æ–‡ä»¶åˆ—è¡¨ä¸­çš„.novåç¼€
    
    Args:
        files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        Tuple[int, List[str]]: (æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°é‡, é”™è¯¯æ¶ˆæ¯åˆ—è¡¨)
    """
    success_count = 0
    error_files = []
    
    # åªå¤„ç†æœ‰.novåç¼€çš„æ–‡ä»¶
    files_to_process = [f for f in files if f.lower().endswith('.nov')]
    
    if not files_to_process:
        return 0, []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
        console=console
    ) as progress:
        task = progress.add_task(f"æ­£åœ¨ç§»é™¤.novæ‰©å±•å...", total=len(files_to_process))
        
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = [executor.submit(process_single_file, file, False) for file in files_to_process]
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(futures):
                result, file_or_error = future.result()
                if result:
                    success_count += 1
                else:
                    error_files.append(file_or_error)
                progress.advance(task)
    
    return success_count, error_files


def execute_operation(scan_data: Dict[str, Any], operation: str, prefix_name: str = "hb", recursive: bool = False) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ“ä½œ
    
    Args:
        scan_data: åŒ…å«æ‰«æç»“æœçš„æ•°æ®å­—å…¸
        operation: æ“ä½œç±»å‹ï¼Œå¯é€‰å€¼ä¸º 'add_nov', 'remove_nov', 'check_duplicates'
        prefix_name: ç”¨äºæ£€æŸ¥é‡å¤é¡¹çš„å‰ç¼€åç§°ï¼Œé»˜è®¤ä¸º"hb"
        recursive: æ˜¯å¦é€’å½’å¤„ç†å­æ–‡ä»¶å¤¹
        
    Returns:
        Dict[str, Any]: æ‰§è¡Œç»“æœå­—å…¸
    """
    scan_results = scan_data.get("scan_results", {})
    result = {
        "operation": operation,
        "success_count": 0,
        "error_count": 0,
        "errors": [],
        "paths_processed": list(scan_results.keys()),
        "recursive": recursive,
    }
    
    if operation == 'check_duplicates':
        # é€‰æ‹©ä¸€ä¸ªè¾“å‡ºç›®å½•ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè·¯å¾„ï¼‰
        if scan_results:
            output_dir = next(iter(scan_results.keys()))
            
            # åˆå¹¶æ‰€æœ‰è·¯å¾„çš„æ–‡ä»¶åˆ—è¡¨
            merged_scan_results = {
                "nov_files": [],
                "normal_files": [],
                "prefixed_files": {}
            }
            
            # åˆå§‹åŒ–å‰ç¼€æ–‡ä»¶å®¹å™¨
            prefixes = get_prefix_list()
            for prefix_info in prefixes:
                prefix_name = prefix_info.get("name")
                merged_scan_results["prefixed_files"][prefix_name] = []
              # åˆå¹¶æ‰€æœ‰è·¯å¾„çš„æ‰«æç»“æœ
            for path, data in scan_results.items():
                merged_scan_results["nov_files"].extend(data.get("nov_files", []))
                merged_scan_results["normal_files"].extend(data.get("normal_files", []))
                
                # åˆå¹¶å‰ç¼€æ–‡ä»¶
                for name, prefixed_files in data.get("prefixed_files", {}).items():
                    merged_scan_results["prefixed_files"][name].extend(prefixed_files)
            
            # ä½¿ç”¨ä¼ å…¥çš„å‰ç¼€åç§°è¿›è¡Œæ£€æŸ¥
            dup_result = check_and_save_duplicates(output_dir, merged_scan_results, prefix_name)
            result["output_dir"] = output_dir
            # å¢å¼ºçš„è¿”å›ä¿¡æ¯ï¼Œä¾¿äºåç»­å¤„ç†æˆ–æµ‹è¯•
            result["duplicate_count"] = len(dup_result.get("duplicates", []))
            result["pairs_count"] = len(dup_result.get("pairs", []))
            result["prefixed_larger_count"] = len(dup_result.get("prefixed_larger", []))
            result["prefixed_larger"] = dup_result.get("prefixed_larger", [])
    
    elif operation == 'add_nov':
        start_time = time.time()
        all_normal_files = []
        
        for path, data in scan_results.items():
            all_normal_files.extend(data.get("normal_files", []))
        
        # æ‰¹é‡æ·»åŠ .novåç¼€
        if all_normal_files:
            console.print(f"\n[blue]ğŸ”„ å¼€å§‹æ·»åŠ .novåç¼€ï¼Œå¤„ç† {len(all_normal_files)} ä¸ªæ–‡ä»¶...[/blue]")
            success_count, errors = add_nov_extension_to_files(all_normal_files)
            result["success_count"] = success_count
            result["error_count"] = len(errors)
            result["errors"] = errors
        else:
            console.print("\n[yellow]âš ï¸ æœªæ‰¾åˆ°éœ€è¦æ·»åŠ .novåç¼€çš„æ™®é€šè§†é¢‘æ–‡ä»¶ï¼[/yellow]")
        
        end_time = time.time()
        result["execution_time"] = end_time - start_time
    
    elif operation == 'remove_nov':
        start_time = time.time()
        all_nov_files = []
        
        for path, data in scan_results.items():
            all_nov_files.extend(data.get("nov_files", []))
        
        # æ‰¹é‡ç§»é™¤.novåç¼€
        if all_nov_files:
            console.print(f"\n[blue]ğŸ”„ å¼€å§‹ç§»é™¤.novåç¼€ï¼Œå¤„ç† {len(all_nov_files)} ä¸ªæ–‡ä»¶...[/blue]")
            success_count, errors = remove_nov_extension_from_files(all_nov_files)
            result["success_count"] = success_count
            result["error_count"] = len(errors)
            result["errors"] = errors
        else:
            console.print("\n[yellow]âš ï¸ æœªæ‰¾åˆ°å¸¦.novåç¼€çš„è§†é¢‘æ–‡ä»¶ï¼[/yellow]")
        
        end_time = time.time()
        result["execution_time"] = end_time - start_time
    
    # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
    if operation in ('add_nov', 'remove_nov') and result.get("success_count", 0) > 0:
        console.print(Panel.fit(f"[bold green]å¤„ç†å®Œæˆï¼[/bold green]"))
        console.print(f"[green]âœ“ æˆåŠŸå¤„ç†:[/green] {result['success_count']} ä¸ªæ–‡ä»¶")
        
        if "execution_time" in result:
            console.print(f"[blue]ğŸ“Š æ€»è€—æ—¶:[/blue] {result['execution_time']:.2f} ç§’")
        
        if result.get("errors", []):
            console.print("[yellow]âš ï¸ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°ä»¥ä¸‹é”™è¯¯:[/yellow]")
            for error in result["errors"]:
                console.print(f"  [red]â€¢ {error}[/red]")
    
    return result
